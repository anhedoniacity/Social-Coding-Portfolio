{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPHj6tbNUTYtQaddgX18ay3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"qc_PvRG2zWyv","executionInfo":{"status":"ok","timestamp":1726776511543,"user_tz":240,"elapsed":139,"user":{"displayName":"El Loprest","userId":"09345292030030117596"}}},"outputs":[],"source":["# Import the necessary libraries to begin the script\n","import re  # Regular expressions for text processing\n","from collections import Counter  # Count occurrences of elements like words\n","\n","# Define the function to split up a text into individual words\n","def split_into_words(any_chunk_of_text):\n","    lowercase_text = any_chunk_of_text.lower()  # Convert the text to lowercase to ensure uniformity\n","    split_words = re.split(r\"\\W+\", lowercase_text)  # Use regular expressions to split the text by any non-word character\n","    return split_words  # Return the list of words"]},{"cell_type":"code","source":["# Define the filepath and assign variables\n","novel_file = \"/content/Alice.txt\"\n","\n","number_of_desired_words = 20\n","\n","# Create a list of stopwords\n","stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n"," 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n"," 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n"," 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n"," 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n"," 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n"," 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n"," 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n"," 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'electronic', 'archive', 'foundation',\n"," 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'mr',\n"," 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'tm', 'said', 'www', 'org',\n"," 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp', 'gutenberg', 'project']\n","\n","# Read in the file\n","novel_text = open(novel_file, encoding=\"utf-8\").read()\n"],"metadata":{"id":"-bhlPJwi5pjf","executionInfo":{"status":"ok","timestamp":1726776931295,"user_tz":240,"elapsed":169,"user":{"displayName":"El Loprest","userId":"09345292030030117596"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Tokenize and count frequency of meaningful terms in google nest's terms of service or privacy policy\n","all_of_words = split_into_words(novel_text)\n","meaningful_words = [word for word in all_of_words if word not in stopwords]\n","meaningful_words_tally = Counter(meaningful_words)\n","most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n","\n","# Output results\n","most_frequent_meaningful_words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t_dY8VrM6LMM","executionInfo":{"status":"ok","timestamp":1726776934940,"user_tz":240,"elapsed":160,"user":{"displayName":"El Loprest","userId":"09345292030030117596"}},"outputId":"d841e890-b8ed-4ce9-ef81-8a9f6101979e"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('alice', 402),\n"," ('little', 127),\n"," ('one', 103),\n"," ('know', 88),\n"," ('like', 85),\n"," ('went', 83),\n"," ('would', 78),\n"," ('queen', 76),\n"," ('could', 75),\n"," ('thought', 74),\n"," ('time', 71),\n"," ('see', 70),\n"," ('well', 63),\n"," ('king', 63),\n"," ('m', 60),\n"," ('turtle', 59),\n"," ('way', 58),\n"," ('mock', 57),\n"," ('began', 57),\n"," ('hatter', 56)]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["from nltk import bigrams  # Import NLTK's bigrams function\n","\n","# Generate bigrams (collocations) from meaningful words\n","word_bigrams = list(bigrams(meaningful_words))\n","\n","# Count the frequency of bigrams\n","bigrams_tally = Counter(word_bigrams)\n","most_frequent_bigrams = bigrams_tally.most_common(20)\n","\n","# Output results\n","most_frequent_bigrams"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlZoAZYe6b9i","executionInfo":{"status":"ok","timestamp":1726776937963,"user_tz":240,"elapsed":168,"user":{"displayName":"El Loprest","userId":"09345292030030117596"}},"outputId":"25128349-5513-4f89-f4a4-f49e9abc530c"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('mock', 'turtle'), 57),\n"," (('march', 'hare'), 31),\n"," (('thought', 'alice'), 27),\n"," (('white', 'rabbit'), 22),\n"," (('1', 'e'), 22),\n"," (('united', 'states'), 15),\n"," (('alice', 'thought'), 13),\n"," (('minute', 'two'), 12),\n"," (('poor', 'alice'), 11),\n"," (('alice', 'could'), 11),\n"," (('1', 'f'), 11),\n"," (('oh', 'dear'), 10),\n"," (('paragraph', '1'), 10),\n"," (('poor', 'little'), 9),\n"," (('might', 'well'), 9),\n"," (('alice', 'went'), 9),\n"," (('alice', 'replied'), 9),\n"," (('alice', 'looked'), 9),\n"," (('join', 'dance'), 9),\n"," (('terms', 'agreement'), 9)]"]},"metadata":{},"execution_count":24}]}]}